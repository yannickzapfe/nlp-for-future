{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dCpvgG0vwXAZ"
   },
   "source": [
    "# Predicting News Category With BERT IN Tensorflow\n",
    "\n",
    "---\n",
    "\n",
    "Bidirectional Encoder Representations from Transformers or BERT for short is a very popular NLP model from Google known for producing state-of-the-art results in a wide variety of NLP tasks.\n",
    "\n",
    "The importance of Natural Language Processing(NLP) is profound in the Artificial Intelligence domain. The most abundant data in the world today is in the form of texts and having a powerful text processing system is critical and is more than  just a necessity.\n",
    "\n",
    "In this article we look at implementing a multi-class classification using the state-of-the-art model, BERT.\n",
    "\n",
    "---\n",
    "\n",
    "##### Pre-Requisites:\n",
    "\n",
    "##### An Understanding of BERT\n",
    "---\n",
    "\n",
    "## About Dataset\n",
    "\n",
    "For this article, we will use MachineHackâ€™s Predict The News Category Hackathon data. The data  consists of a collection of news articles which are categorized into four sections. The features of the datasets are as follows:\n",
    "\n",
    "Size of training set: 7,628 records\n",
    "Size of test set: 2,748 records\n",
    "\n",
    "FEATURES:\n",
    "\n",
    "STORY:  A part of the main content of the article to be published as a piece of news.\n",
    "SECTION: The genre/category the STORY falls in.\n",
    "\n",
    "There are four distinct sections where each story may fall in to. The Sections are labelled as follows :\n",
    "Politics: 0\n",
    "Technology: 1\n",
    "Entertainment: 2\n",
    "Business: 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HAssmxxJp0yM"
   },
   "source": [
    "## Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "hsZvic2YxnTz",
    "outputId": "124ac0c4-df1e-40f4-a29f-68995302ac1e"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import bert\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kyzTzLpyqJUf"
   },
   "source": [
    "## Setting The Output Directory\n",
    "---\n",
    "While fine-tuning the model, we will save the training checkpoints and the model in an output directory so that we can use the trained model for our predictions later.\n",
    "\n",
    "The following code block sets an output directory :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "US_EAnICvP7f",
    "outputId": "ff060b68-a834-4e85-bd9f-54c2760c04e8"
   },
   "outputs": [],
   "source": [
    "OUTPUT_DIR = 'bert'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pmFYvkylMwXn"
   },
   "source": [
    "## Loading The Data\n",
    "---\n",
    "We will now load the data from a Google Drive directory and will also split the training set in to training and validation sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VIsetAbCam6y"
   },
   "outputs": [],
   "source": [
    "DATA_SIZE = 1000\n",
    "data = pd.read_csv(\"../nlp-for-future-data/reviews\" + str(DATA_SIZE) + \".csv\")\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "e_rukDBlbvCj",
    "outputId": "50f8f587-97cd-457f-b151-cf2b33adbc9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Shape : (800, 3)\n",
      "Test Set Shape : (200, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Set Shape :\", train.shape)\n",
    "print(\"Test Set Shape :\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IuMOGwFui4it"
   },
   "outputs": [],
   "source": [
    "DATA_COLUMN = 'review'\n",
    "LABEL_COLUMN = 'score'\n",
    "label_list = list(train.score.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V399W0rqNJ-Z"
   },
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "BERT model accept only a specific type of input and the datasets are usually structured to have have the following four features:\n",
    "\n",
    "* guid : A unique id that represents an observation.\n",
    "* text_a : The text we need to classify into given categories\n",
    "* text_b: It is used when we're training a model to understand the relationship between sentences and it does not apply for classification problems.\n",
    "* label: It consists of the labels or classes or categories that a given text belongs to.\n",
    " \n",
    "In our dataset we have text_a and label. The following code block will create objects for each of the above mentioned features for all the records in our dataset using the InputExample class provided in the BERT library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p9gEt5SmM6i6"
   },
   "outputs": [],
   "source": [
    "train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None,\n",
    "                                                                             text_a=x[DATA_COLUMN],\n",
    "                                                                             text_b=None,\n",
    "                                                                             label=x[LABEL_COLUMN]), axis=1)\n",
    "\n",
    "test_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None,\n",
    "                                                                          text_a=x[DATA_COLUMN],\n",
    "                                                                          text_b=None,\n",
    "                                                                          label=x[LABEL_COLUMN]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "id": "a7UC2dnVRsoZ",
    "outputId": "105924ba-0e83-4f65-f7c9-ea7089866043"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 0 - guid of training set :  None\n",
      "\n",
      "__________\n",
      "Row 0 - text_a of training set :  I found this book to be exceptionally frustrating to read. My frustration did not stem from the author's ability to create a solid story. In fact, it is very evident from the text that Caldwell is an incredibly gifted writer. His depiction of the protagonist being tortured put chills down my spine. The twists of logic that were expressed by the torturer in those scenes were exactly what one would imagine someone in that position saying to justify their behavior. Caldwell is also able to show in his writing that he has a strong understanding of the philosophical and theological arguments used to support a Christian lifestyle.Given his ability to write meaningful narrative and realistic dialogue, it is tremendously disappointing that Caldwell chose to waste his gifts by creating such an unlikable protagonist. I recognize that having a central character with a &quot;fatal flaw&quot; is a standard literary device. However, the device usually makes people feel sympathy when that character meets their fate. Such emotion cannot be generated for the main character in We All Fall Down. It would seem impossible that anyone who recognizes the situation depicted in the book would behave the way he does. While the language didn't bother me as much as other reviewers, the graphic acts of violence seemed totally out of place for a man who understood that he was living through the Apocalypse. As a result, I wasn't troubled by his fate: I was pleased, because he got what he deserved. Sadly, the book becomes unenjoyable because it is centered around this unlikable character..... I'm sure the immensely talented Mr. Caldwell will produce some masterful works in the future. However, I'm also sure that We All Fall Down will be noted as an ambitious, yet fatally flawed, debut.\n",
      "\n",
      "__________\n",
      "Row 0 - text_b of training set :  None\n",
      "\n",
      "__________\n",
      "Row 0 - label of training set :  2.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Row 0 - guid of training set : \", train_InputExamples.iloc[0].guid)\n",
    "print(\"\\n__________\\nRow 0 - text_a of training set : \", train_InputExamples.iloc[0].text_a)\n",
    "print(\"\\n__________\\nRow 0 - text_b of training set : \", train_InputExamples.iloc[0].text_b)\n",
    "print(\"\\n__________\\nRow 0 - label of training set : \", train_InputExamples.iloc[0].label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qMWiDtpyQSoU"
   },
   "source": [
    "We will now get down to business with the pretrained BERT.  In this example we will use the ```bert_uncased_L-12_H-768_A-12/1``` model. To check all available versions click [here](https://tfhub.dev/s?network-architecture=transformer&publisher=google).\n",
    "\n",
    "We will be using the vocab.txt file in the model to map the words in the dataset to indexes. Also the loaded BERT model is trained on uncased/lowercase data and hence the data we feed to train the model should also be of lowercase.\n",
    "\n",
    "---\n",
    "\n",
    "The following code block loads the pre-trained BERT model and initializers a tokenizer object for tokenizing the texts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "IhJSe0QHNG7U",
    "outputId": "5591de28-d634-4e39-df73-6576d7f4cfb3"
   },
   "outputs": [],
   "source": [
    "# This is a path to an uncased (all lowercase) version of BERT\n",
    "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "\n",
    "\n",
    "def create_tokenizer_from_hub_module():\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    with tf.Graph().as_default():\n",
    "        bert_module = hub.Module(BERT_MODEL_HUB)\n",
    "        tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "        with tf.Session() as sess:\n",
    "            vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                                  tokenization_info[\"do_lower_case\"]])\n",
    "\n",
    "    return bert.tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "\n",
    "tokenizer = create_tokenizer_from_hub_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "t3T3jSpjSxmd",
    "outputId": "bc437cfd-4c1a-4384-b080-0881c99eb34f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'found', 'this', 'book', 'to', 'be', 'exceptionally', 'frustrating', 'to', 'read', '.', 'my', 'frustration', 'did', 'not', 'stem', 'from', 'the', 'author', \"'\", 's', 'ability', 'to', 'create', 'a', 'solid', 'story', '.', 'in', 'fact', ',', 'it', 'is', 'very', 'evident', 'from', 'the', 'text', 'that', 'caldwell', 'is', 'an', 'incredibly', 'gifted', 'writer', '.', 'his', 'depiction', 'of', 'the', 'protagonist', 'being', 'tortured', 'put', 'chill', '##s', 'down', 'my', 'spine', '.', 'the', 'twists', 'of', 'logic', 'that', 'were', 'expressed', 'by', 'the', 'torture', '##r', 'in', 'those', 'scenes', 'were', 'exactly', 'what', 'one', 'would', 'imagine', 'someone', 'in', 'that', 'position', 'saying', 'to', 'justify', 'their', 'behavior', '.', 'caldwell', 'is', 'also', 'able', 'to', 'show', 'in', 'his', 'writing', 'that', 'he', 'has', 'a', 'strong', 'understanding', 'of', 'the', 'philosophical', 'and', 'theological', 'arguments', 'used', 'to', 'support', 'a', 'christian', 'lifestyle', '.', 'given', 'his', 'ability', 'to', 'write', 'meaningful', 'narrative', 'and', 'realistic', 'dialogue', ',', 'it', 'is', 'tremendous', '##ly', 'disappointing', 'that', 'caldwell', 'chose', 'to', 'waste', 'his', 'gifts', 'by', 'creating', 'such', 'an', 'un', '##lika', '##ble', 'protagonist', '.', 'i', 'recognize', 'that', 'having', 'a', 'central', 'character', 'with', 'a', '&', 'quo', '##t', ';', 'fatal', 'flaw', '&', 'quo', '##t', ';', 'is', 'a', 'standard', 'literary', 'device', '.', 'however', ',', 'the', 'device', 'usually', 'makes', 'people', 'feel', 'sympathy', 'when', 'that', 'character', 'meets', 'their', 'fate', '.', 'such', 'emotion', 'cannot', 'be', 'generated', 'for', 'the', 'main', 'character', 'in', 'we', 'all', 'fall', 'down', '.', 'it', 'would', 'seem', 'impossible', 'that', 'anyone', 'who', 'recognizes', 'the', 'situation', 'depicted', 'in', 'the', 'book', 'would', 'behave', 'the', 'way', 'he', 'does', '.', 'while', 'the', 'language', 'didn', \"'\", 't', 'bother', 'me', 'as', 'much', 'as', 'other', 'reviewers', ',', 'the', 'graphic', 'acts', 'of', 'violence', 'seemed', 'totally', 'out', 'of', 'place', 'for', 'a', 'man', 'who', 'understood', 'that', 'he', 'was', 'living', 'through', 'the', 'apocalypse', '.', 'as', 'a', 'result', ',', 'i', 'wasn', \"'\", 't', 'troubled', 'by', 'his', 'fate', ':', 'i', 'was', 'pleased', ',', 'because', 'he', 'got', 'what', 'he', 'deserved', '.', 'sadly', ',', 'the', 'book', 'becomes', 'une', '##n', '##joy', '##able', 'because', 'it', 'is', 'centered', 'around', 'this', 'un', '##lika', '##ble', 'character', '.', '.', '.', '.', '.', 'i', \"'\", 'm', 'sure', 'the', 'immensely', 'talented', 'mr', '.', 'caldwell', 'will', 'produce', 'some', 'master', '##ful', 'works', 'in', 'the', 'future', '.', 'however', ',', 'i', \"'\", 'm', 'also', 'sure', 'that', 'we', 'all', 'fall', 'down', 'will', 'be', 'noted', 'as', 'an', 'ambitious', ',', 'yet', 'fatally', 'flawed', ',', 'debut', '.']\n"
     ]
    }
   ],
   "source": [
    "#Here is what the tokenised sample of the first training set observation looks like.\n",
    "print(tokenizer.tokenize(train_InputExamples.iloc[0].text_a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mtvrR5eusZPO"
   },
   "source": [
    "We will now format out text in to input features which the BERT model expects. We will also set a sequence length which will be the length of the input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "LL5W8gEGRTAf",
    "outputId": "ae954549-b82e-412c-9772-168a3664a4e2"
   },
   "outputs": [],
   "source": [
    "# We'll set sequences to be at most 128 tokens long.\n",
    "MAX_SEQ_LENGTH = 128\n",
    "\n",
    "# Convert our train and test features to InputFeatures that BERT understands.\n",
    "train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples,\n",
    "                                                                  label_list,\n",
    "                                                                  MAX_SEQ_LENGTH,\n",
    "                                                                  tokenizer)\n",
    "\n",
    "test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples,\n",
    "                                                                 label_list,\n",
    "                                                                 MAX_SEQ_LENGTH,\n",
    "                                                                 tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "id": "WZEmm8KEUX3F",
    "outputId": "f8fa6a4c-7282-4469-8b2e-7f5243139ddc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence :  I found this book to be exceptionally frustrating to read. My frustration did not stem from the author's ability to create a solid story. In fact, it is very evident from the text that Caldwell is an incredibly gifted writer. His depiction of the protagonist being tortured put chills down my spine. The twists of logic that were expressed by the torturer in those scenes were exactly what one would imagine someone in that position saying to justify their behavior. Caldwell is also able to show in his writing that he has a strong understanding of the philosophical and theological arguments used to support a Christian lifestyle.Given his ability to write meaningful narrative and realistic dialogue, it is tremendously disappointing that Caldwell chose to waste his gifts by creating such an unlikable protagonist. I recognize that having a central character with a &quot;fatal flaw&quot; is a standard literary device. However, the device usually makes people feel sympathy when that character meets their fate. Such emotion cannot be generated for the main character in We All Fall Down. It would seem impossible that anyone who recognizes the situation depicted in the book would behave the way he does. While the language didn't bother me as much as other reviewers, the graphic acts of violence seemed totally out of place for a man who understood that he was living through the Apocalypse. As a result, I wasn't troubled by his fate: I was pleased, because he got what he deserved. Sadly, the book becomes unenjoyable because it is centered around this unlikable character..... I'm sure the immensely talented Mr. Caldwell will produce some masterful works in the future. However, I'm also sure that We All Fall Down will be noted as an ambitious, yet fatally flawed, debut.\n",
      "------------------------------\n",
      "Tokens :  ['i', 'found', 'this', 'book', 'to', 'be', 'exceptionally', 'frustrating', 'to', 'read', '.', 'my', 'frustration', 'did', 'not', 'stem', 'from', 'the', 'author', \"'\", 's', 'ability', 'to', 'create', 'a', 'solid', 'story', '.', 'in', 'fact', ',', 'it', 'is', 'very', 'evident', 'from', 'the', 'text', 'that', 'caldwell', 'is', 'an', 'incredibly', 'gifted', 'writer', '.', 'his', 'depiction', 'of', 'the', 'protagonist', 'being', 'tortured', 'put', 'chill', '##s', 'down', 'my', 'spine', '.', 'the', 'twists', 'of', 'logic', 'that', 'were', 'expressed', 'by', 'the', 'torture', '##r', 'in', 'those', 'scenes', 'were', 'exactly', 'what', 'one', 'would', 'imagine', 'someone', 'in', 'that', 'position', 'saying', 'to', 'justify', 'their', 'behavior', '.', 'caldwell', 'is', 'also', 'able', 'to', 'show', 'in', 'his', 'writing', 'that', 'he', 'has', 'a', 'strong', 'understanding', 'of', 'the', 'philosophical', 'and', 'theological', 'arguments', 'used', 'to', 'support', 'a', 'christian', 'lifestyle', '.', 'given', 'his', 'ability', 'to', 'write', 'meaningful', 'narrative', 'and', 'realistic', 'dialogue', ',', 'it', 'is', 'tremendous', '##ly', 'disappointing', 'that', 'caldwell', 'chose', 'to', 'waste', 'his', 'gifts', 'by', 'creating', 'such', 'an', 'un', '##lika', '##ble', 'protagonist', '.', 'i', 'recognize', 'that', 'having', 'a', 'central', 'character', 'with', 'a', '&', 'quo', '##t', ';', 'fatal', 'flaw', '&', 'quo', '##t', ';', 'is', 'a', 'standard', 'literary', 'device', '.', 'however', ',', 'the', 'device', 'usually', 'makes', 'people', 'feel', 'sympathy', 'when', 'that', 'character', 'meets', 'their', 'fate', '.', 'such', 'emotion', 'cannot', 'be', 'generated', 'for', 'the', 'main', 'character', 'in', 'we', 'all', 'fall', 'down', '.', 'it', 'would', 'seem', 'impossible', 'that', 'anyone', 'who', 'recognizes', 'the', 'situation', 'depicted', 'in', 'the', 'book', 'would', 'behave', 'the', 'way', 'he', 'does', '.', 'while', 'the', 'language', 'didn', \"'\", 't', 'bother', 'me', 'as', 'much', 'as', 'other', 'reviewers', ',', 'the', 'graphic', 'acts', 'of', 'violence', 'seemed', 'totally', 'out', 'of', 'place', 'for', 'a', 'man', 'who', 'understood', 'that', 'he', 'was', 'living', 'through', 'the', 'apocalypse', '.', 'as', 'a', 'result', ',', 'i', 'wasn', \"'\", 't', 'troubled', 'by', 'his', 'fate', ':', 'i', 'was', 'pleased', ',', 'because', 'he', 'got', 'what', 'he', 'deserved', '.', 'sadly', ',', 'the', 'book', 'becomes', 'une', '##n', '##joy', '##able', 'because', 'it', 'is', 'centered', 'around', 'this', 'un', '##lika', '##ble', 'character', '.', '.', '.', '.', '.', 'i', \"'\", 'm', 'sure', 'the', 'immensely', 'talented', 'mr', '.', 'caldwell', 'will', 'produce', 'some', 'master', '##ful', 'works', 'in', 'the', 'future', '.', 'however', ',', 'i', \"'\", 'm', 'also', 'sure', 'that', 'we', 'all', 'fall', 'down', 'will', 'be', 'noted', 'as', 'an', 'ambitious', ',', 'yet', 'fatally', 'flawed', ',', 'debut', '.']\n",
      "------------------------------\n",
      "Input IDs :  [101, 1045, 2179, 2023, 2338, 2000, 2022, 17077, 25198, 2000, 3191, 1012, 2026, 9135, 2106, 2025, 7872, 2013, 1996, 3166, 1005, 1055, 3754, 2000, 3443, 1037, 5024, 2466, 1012, 1999, 2755, 1010, 2009, 2003, 2200, 10358, 2013, 1996, 3793, 2008, 16589, 2003, 2019, 11757, 12785, 3213, 1012, 2010, 15921, 1997, 1996, 10191, 2108, 12364, 2404, 10720, 2015, 2091, 2026, 8560, 1012, 1996, 21438, 1997, 7961, 2008, 2020, 5228, 2011, 1996, 8639, 2099, 1999, 2216, 5019, 2020, 3599, 2054, 2028, 2052, 5674, 2619, 1999, 2008, 2597, 3038, 2000, 16114, 2037, 5248, 1012, 16589, 2003, 2036, 2583, 2000, 2265, 1999, 2010, 3015, 2008, 2002, 2038, 1037, 2844, 4824, 1997, 1996, 9569, 1998, 9208, 9918, 2109, 2000, 2490, 1037, 3017, 9580, 1012, 2445, 2010, 3754, 2000, 4339, 15902, 7984, 1998, 102]\n",
      "------------------------------\n",
      "Input Masks :  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "------------------------------\n",
      "Segment IDs :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "#Example on first observation in the training set\n",
    "print(\"Sentence : \", train_InputExamples.iloc[0].text_a)\n",
    "print(\"-\" * 30)\n",
    "print(\"Tokens : \", tokenizer.tokenize(train_InputExamples.iloc[0].text_a))\n",
    "print(\"-\" * 30)\n",
    "print(\"Input IDs : \", train_features[0].input_ids)\n",
    "print(\"-\" * 30)\n",
    "print(\"Input Masks : \", train_features[0].input_mask)\n",
    "print(\"-\" * 30)\n",
    "print(\"Segment IDs : \", train_features[0].segment_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ccp5trMwRtmr"
   },
   "source": [
    "## Creating A Multi-Class Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6o2a5ZIvRcJq"
   },
   "outputs": [],
   "source": [
    "def create_model(is_predicting, input_ids, input_mask, segment_ids, labels, num_labels):\n",
    "    bert_module = hub.Module(BERT_MODEL_HUB, trainable=True)\n",
    "    bert_inputs = dict(input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids)\n",
    "    bert_outputs = bert_module(inputs=bert_inputs, signature=\"tokens\", as_dict=True)\n",
    "\n",
    "    # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
    "    # Use \"sequence_outputs\" for token-level output.\n",
    "    output_layer = bert_outputs[\"pooled_output\"]\n",
    "\n",
    "    hidden_size = output_layer.shape[-1].value\n",
    "\n",
    "    # Create our own layer to tune for politeness data.\n",
    "    output_weights = tf.get_variable(\n",
    "        \"output_weights\", [num_labels, hidden_size],\n",
    "        initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "    output_bias = tf.get_variable(\"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
    "\n",
    "    with tf.variable_scope(\"loss\"):\n",
    "        # Dropout helps prevent overfitting\n",
    "        output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
    "\n",
    "        logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
    "        logits = tf.nn.bias_add(logits, output_bias)\n",
    "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "\n",
    "        # Convert labels into one-hot encoding\n",
    "        one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
    "\n",
    "        predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
    "        # If we're predicting, we want predicted labels and the probabiltiies.\n",
    "        if is_predicting:\n",
    "            return (predicted_labels, log_probs)\n",
    "\n",
    "        # If we're train/eval, compute loss between predicted and actual label\n",
    "        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "        loss = tf.reduce_mean(per_example_loss)\n",
    "        return (loss, predicted_labels, log_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FnH-AnOQ9KKW"
   },
   "outputs": [],
   "source": [
    "#A function that adapts our model to work for training, evaluation, and prediction.\n",
    "\n",
    "# model_fn_builder actually creates our model function\n",
    "# using the passed parameters for num_labels, learning_rate, etc.\n",
    "def model_fn_builder(num_labels, learning_rate, num_train_steps,\n",
    "                     num_warmup_steps):\n",
    "    \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
    "\n",
    "    def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
    "        \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
    "\n",
    "        input_ids = features[\"input_ids\"]\n",
    "        input_mask = features[\"input_mask\"]\n",
    "        segment_ids = features[\"segment_ids\"]\n",
    "        label_ids = features[\"label_ids\"]\n",
    "\n",
    "        is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
    "\n",
    "        # TRAIN and EVAL\n",
    "        if not is_predicting:\n",
    "\n",
    "            (loss, predicted_labels, log_probs) = create_model(\n",
    "                is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
    "\n",
    "            train_op = bert.optimization.create_optimizer(\n",
    "                loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
    "\n",
    "            # Calculate evaluation metrics.\n",
    "            def metric_fn(label_ids, predicted_labels):\n",
    "                accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
    "                mse = tf.metrics.mean_squared_error(label_ids, predicted_labels)\n",
    "\n",
    "                return {\n",
    "                    \"Accuracy\": accuracy,\n",
    "                    \"MSE\": mse,\n",
    "                }\n",
    "\n",
    "            eval_metrics = metric_fn(label_ids, predicted_labels)\n",
    "\n",
    "            if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "                return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "            else:\n",
    "                return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metrics)\n",
    "        else:\n",
    "            (predicted_labels, log_probs) = create_model(\n",
    "                is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
    "\n",
    "            predictions = {\n",
    "                'probabilities': log_probs,\n",
    "                'labels': predicted_labels\n",
    "            }\n",
    "            return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "    # Return the actual model function in the closure\n",
    "    return model_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OjwJ4bTeWXD8"
   },
   "outputs": [],
   "source": [
    "# Compute train and warmup steps from batch size\n",
    "# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_TRAIN_EPOCHS = 3.0\n",
    "# Warmup is a period of time where the learning rate is small and gradually increases--usually helps training.\n",
    "WARMUP_PROPORTION = 0.1\n",
    "# Model configs\n",
    "SAVE_CHECKPOINTS_STEPS = 300\n",
    "SAVE_SUMMARY_STEPS = 100\n",
    "\n",
    "# Compute train and warmup steps from batch size\n",
    "num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
    "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
    "\n",
    "# Specify output directory and number of checkpoint steps to save\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    model_dir=OUTPUT_DIR,\n",
    "    save_summary_steps=SAVE_SUMMARY_STEPS,\n",
    "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "colab_type": "code",
    "id": "q_WebpS1X97v",
    "outputId": "917d1322-1812-42d6-901c-46798a121652"
   },
   "outputs": [],
   "source": [
    "#Initializing the model and the estimator\n",
    "model_fn = model_fn_builder(\n",
    "    num_labels=len(label_list),\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_steps=num_train_steps,\n",
    "    num_warmup_steps=num_warmup_steps)\n",
    "\n",
    "estimator = tf.estimator.Estimator(\n",
    "    model_fn=model_fn,\n",
    "    config=run_config,\n",
    "    params={\"batch_size\": BATCH_SIZE})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NOO3RfG1DYLo"
   },
   "source": [
    "we will now create an input builder function that takes our training feature set (`train_features`) and produces a generator. This is a pretty standard design pattern for working with Tensorflow [Estimators](https://www.tensorflow.org/guide/estimators)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Pv2bAlOX_-K"
   },
   "outputs": [],
   "source": [
    "# Create an input function for training. drop_remainder = True for using TPUs.\n",
    "train_input_fn = bert.run_classifier.input_fn_builder(\n",
    "    features=train_features,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=True,\n",
    "    drop_remainder=False)\n",
    "\n",
    "# Create an input function for validating. drop_remainder = True for using TPUs.\n",
    "test_input_fn = bert.run_classifier.input_fn_builder(\n",
    "    features=test_features,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=False,\n",
    "    drop_remainder=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_vrumsg9uygH"
   },
   "source": [
    "## Training & Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "nucD4gluYJmK",
    "outputId": "7793bdd3-8be5-4f9c-9c70-de03397186ad",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#Training the model\n",
    "print(f'Beginning Training!')\n",
    "current_time = datetime.now()\n",
    "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "print(\"Training took time \", datetime.now() - current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "colab_type": "code",
    "id": "PPVEXhNjYXC-",
    "outputId": "23deac72-c825-46a9-835b-721bbdef57e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "C:\\Users\\yanni\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2022-10-31T11:10:44Z\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2022-10-31T11:10:44Z\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from bert\\model.ckpt-60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from bert\\model.ckpt-60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished evaluation at 2022-10-31-11:11:45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished evaluation at 2022-10-31-11:11:45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving dict for global step 60: eval_accuracy = 0.4125, false_negatives = 22.0, false_positives = 8.0, global_step = 60, loss = 1.3597462, true_negatives = 25.0, true_positives = 105.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving dict for global step 60: eval_accuracy = 0.4125, false_negatives = 22.0, false_positives = 8.0, global_step = 60, loss = 1.3597462, true_negatives = 25.0, true_positives = 105.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 60: bert\\model.ckpt-60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 60: bert\\model.ckpt-60\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'eval_accuracy': 0.4125,\n 'false_negatives': 22.0,\n 'false_positives': 8.0,\n 'loss': 1.3597462,\n 'true_negatives': 25.0,\n 'true_positives': 105.0,\n 'global_step': 60}"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluating the model with Validation set\n",
    "estimator.evaluate(input_fn=test_input_fn, steps=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qi5MqgDRhZno"
   },
   "source": [
    "# Reference:\n",
    "Most of the code has been taken from the following resource:\n",
    "\n",
    "* https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "wEhTK6Sypwqr",
    "HAssmxxJp0yM",
    "kyzTzLpyqJUf",
    "pmFYvkylMwXn",
    "ccp5trMwRtmr",
    "_vrumsg9uygH"
   ],
   "name": "Predicting_News_Category_with_BERT_in_Tensorflow.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
