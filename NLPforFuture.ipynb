{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# do imports\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# define helper functions\n",
    "\n",
    "# For a given review (in the form of a list or set of tokens), create a\n",
    "# dictionary which tells us which words are present and which are not.\n",
    "def get_review_features(review):\n",
    "    review_words = set(review)\n",
    "    return {word: word in review_words for word in words_map}\n",
    "\n",
    "\n",
    "# flattens two dimensional list\n",
    "def flatten(in_list):\n",
    "    return [word for sent in in_list for word in sent]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "books_data_raw = pd.read_csv('../data/Books_rating.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(books_data_raw)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "books_data = books_data_raw[['review/score', 'review/text']].copy()\n",
    "books_data.rename(columns={'review/score': 'score', 'review/text': 'text'}, inplace=True)\n",
    "\n",
    "# TODO: use more 10000\n",
    "books_data = books_data.head(10000)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "books_data = books_data.dropna()\n",
    "len(books_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# tokenize the review text\n",
    "x = books_data.text.apply(lambda x: nltk.word_tokenize(x))\n",
    "x.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "words = flatten(x.tolist())\n",
    "\n",
    "# remove all the stopwords\n",
    "other_things_to_remove = [\",\", \".\", \"(\", \")\", \"'s\", \"&\"]\n",
    "to_remove = list(stopwords.words('english')) + other_things_to_remove\n",
    "words = filter(lambda i: i not in to_remove, [x.lower() for x in words])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create a frequency distribution for the given words\n",
    "words_freqDist = nltk.FreqDist(words)\n",
    "\n",
    "# and put them into a map\n",
    "words_map = {word for word, count in words_freqDist.most_common(2000)}\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create our pairs of features and target for every review\n",
    "book_review_features = [(get_review_features(review), score) for review, score in zip(x, books_data.score)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# split data into training and test\n",
    "x_train, x_test = train_test_split(book_review_features, train_size=0.8)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# analyze the distribution of our data\n",
    "c = Counter()\n",
    "for item in x_train:\n",
    "    c[item[1]] += 1\n",
    "\n",
    "print(c)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train a naive bayes on the training data and test its accuracy\n",
    "b_naive_bayes = nltk.NaiveBayesClassifier.train(x_train)\n",
    "\n",
    "acc = nltk.classify.accuracy(b_naive_bayes, x_test)\n",
    "\n",
    "print(f\"Accuracy: {round(acc * 100, 2)}%\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
